---
title: 'Harvard Data Science Project: Climate Change Worry'
author: "Dr. Fatih Uenal, Cambridge, UK"
date: "4/27/2021"
output: 
  html_document:
    toc: yes
    toc_depth: 5
    theme: cosmo
    highlight: tango
    code_folding: show
    fig_width: 12
    fig_height: 8
---

```{r general-options-document-format, include=FALSE}
# knitr::opts_chunk$set(echo = F, warning = F, fig.pos ='h', message = F, error = F) # Set knitting options general
options(tinytex.verbose = TRUE)  # debugging
```

```{r setup, include=FALSE}
## Install/Load required packages
# All of the following packages need to be installed to run this document properly
if(!require(devtools)) install.packages("devtools", repos = "http://cran.us.r-project.org")
library(devtools)
if(!require(lares)) devtools::install_github("laresbernardo/lares")
if(!require(pacman)) install.packages("pacman", repos = "http://cran.us.r-project.org")
library(pacman)

pacman::p_load(essurvey, readr, curl, tidyverse, haven, skimr, C50, class, e1071, randomForest, rpart, rpart.plot, 
               partykit, caret, caretEnsemble, lares, GGally, ggplot2, PerformanceAnalytics, fastmatch,
               Metrics, ipred, mlbench, RANN, highcharter)

options(dplyr.summarise.inform = FALSE) # Suppress summary info
```

# 1. Overview

In the following, I am presenting the results of the second part of my **HarvardX PH125.9x Data Science: Capstone Project**. The goal of this full-stack data science project is to download, wrangle, explore, and analyze European Citizens' perceptions of climate change using 10 different Machine Learning (ML) Algorithms (*KNN, naiveBayes, Random Forest, adaBOOST, SVM, Ensemble, ctree, GBM, C5.0, rpart*) based on data from the 8th round European Social Survey (2016). I will also employ hyperparameter tuning (e.g., *tuneGrid* and *tuneLength*) to improve the results.  The response variable (outcome) in this project is "climate change worry", i.e., whether or not individuals in more than 30 European countries are worried about climate change. The response to this variable will be binary ('yes' vs. 'no') and thus, this project deals with a classification problem.

After a contextual introduction, I will start with detailing the process of how data download, selection, and cleaning was done. Then, after splitting the data into *train* and *test* sets, I will proceed with an Exploratory Data Analysis (EDA) in which I explore the train data set and visualize the initial data exploration to gain key insights for modeling. In the Methods & Analysis Section, I will first do some feature engineering beyond the steps undertaken in the initial data selection process (i.e., missing value imputation, removing zero variance features, normalization of data). Afterwards, I will employ several ML algorithms and check to what extent each of the algorithms accurately classifies the response variable. The report thus consists of seven parts:

After this **Overview** section, I will provide some contextual information on the data and classification problem of this project in the **Introduction** section.

The following **Data Download & Preparation** section describes how to download and prepare the data-sets. The data set containing over 500 columns most of which are irrelevant for this project. In this section I therefore also clean the data by removing many of the unnecessary columns such as 'administrative' columns from the data-set and describe how I arrived at the final, cleaned data set. 

The following section **Exploratory Data Analysis (EDA)** describes the performed exploratory data analysis and provides an overview of the data and first approaches to optimize it for the machine learning algorithms. Before I begin with the EDA, the **trainData**- and **testData** sets are generated.

In the **Methods and Analysis** section, I will first perform some feature engineering using several pre-processing steps. After the pre-processing, I will run several ML Algorithms and employ hyperparameter tuning to some of them. Next, I will apply the same pre-processing steps on the **testData** set, and compare the accuracy of each trained model on the test set. 

In the **Results** section, I will briefly present the results of the model comparison on the test set and determine the best ML model using appropriate model evaluation metrics. 

The report ends with a **Conclusion** section presenting the main findings, and providing some insights into the limitations of the analysis as well as discussing possible future directions.


---


# 2. Introduction

Reports on the impact of climate change on planetary health are distressing (e.g., IPCC, 2014). Yet, despite being one of the most important anthropogenic societal challenges of our time, public perceptions of and engagement with climate change reveals great variation in regard to acknowledging the urgency and severity of the issue. A growing body of results from an interdisciplinary field spanning psychology, sociology, economics, and communication has identified a wide array of individual predictors of climate change perceptions and attitudes. Nevertheless, practical research limitations—such as limited degrees of freedom using traditional statistics methods—has prevented researchers from directly comparing or meaningfully aggregating the predictive power of these predictors.

In this project, I am going to make use of ML algorithms to overcome the shortcomings of previous studies using data from the 8th Round of the European Social Survey (2016). "The European Social Survey (ESS) is an academically driven cross-national survey that has been conducted across Europe since its establishment in 2001. Every two years, face-to-face interviews are conducted with newly selected, cross-sectional samples. The survey measures the attitudes, beliefs and behaviour patterns of diverse populations in more than thirty nations." The 8th Round of the ESS contains climate change attitudes and a wide of predictor variable of interest. Using this data, I will employ several ML techniques to accurately predict whether or not individuals in this survey are either worried or not worried by climate change and identify which predictors ('features') are the most robust and important factors explaining differences in climate change worry. 


---


# 3. Data Download & Preparation

### 3.1 Data Access  {.tabset}

#### Data Access Route 1

The data set is publicly available following the below link:

 * [European Social Survey 8 (2016)](https://www.europeansocialsurvey.org/data/download.html?r=8) 

#### Data Access Route 2

Alternatively to downloading from the above the link, one can also use the 'essurvey' package. A description on this package and its' usage is available in the below link:

 * ['essurvey'](https://www.r-bloggers.com/2014/03/analyze-the-european-social-survey-ess-with-r/)
 
### 3.2 Download

For this project, I used the second method, and downloaded the data using the 'essurvey'. Please note that you will need to first set an account with ESS and than use your email address to be able to download the data if you choose to use the 'essurvey'. The code to download the data via the 'essurvey' package. is in the accompanying R script. However, due to privacy concerns, I am not using this method in this project. Instead, the data is available in my github repository. I have also uploaded it into the edX repository. Executing the current file should automatically download the data from my github. If that does not work, than please download the data from edX and load it locally.

```{r data-download, warning=FALSE, message=FALSE, echo=FALSE}
## Download ESS Data
# Set email for access
# set_email("ENTER YOUR EMAIL ADRESS HERE")

## Download all countries round 8
# df8 <- import_rounds(8)


## Load data file locally if you download the data from edX
# df8 <- read.csv("ESS8e02.1_F1.csv") 


## Download data file from my github
df8 <- read.csv(curl('https://raw.githubusercontent.com/FUenal/Harvard_Capstone_Project/main/ESS8e02.1_F1.csv'))
```

*PLEASE NOTE: The code is included in the RMD file but explicitly tuned off (echo=FALSE)*.

### 3.3 Preparation
As mentioned in the **Overview** section, the raw data set contains many variables which are irrelevant to this project. As visible below, the raw data set contains 534 columns each representing one variable corresponding to a question asked in the survey or an administrative question and 44387 rows, each corresponding to one surveyed person.

```{r data-peak-1, warning=FALSE, message=FALSE, echo=FALSE}
## First peak into raw data
df_str(df8)
# df_str(df8, return = "skimr")
```

It is also visible that the raw data set contains many missing values as well as different types of variable (e.g., integer, character, etc.). It is clear at this point that data selection and cleaning will require studying the survey documentation prior to anything else. The Survey documentation is very detailed and thorough and can be accessed following this link:

 * [ESS 8 Study Documentation](https://www.europeansocialsurvey.org/docs/round8/survey/ESS8_data_protocol_e01_4.pdf)
 
Having studied the data documentation, I decided to remove many columns from the data-set. I explain my choices below:

 * Since I am interested in analyzing "climate worry" across countries and not for each country individually, the country specific variables are of no relevance for my goal. So I am going to remove the country specific variables all together. 
 
 * For similar reasons as above, I also don't need the sampling stratification weights and population weights and will remove those from the data set as well. 
 
 * The data set also contains a number of so-called 'administrative variables' such as interview time and date etc which are not relevant for my goals and will thus be removed. 
 
 * The survey documentation also list several binary type variables which indicate no relevance to this projects main objective and I will thus remove those as well.
 
 * Lastly, the survey contains many missing values which are coded in many different ways. For example, missing values refer to invalid answers (e.g. refusal, don't know, missing) which are  encoded with numbers 66, 77, 99 for some features, and with 6, 7, 8, 9 for the other features.  Generally, there are a lot of different ways of how the question encoding was designed. This may be the result of many different groups of people working on this survey. I will make use of the 'esssurvey' function to automatically label all invalid answers into NaN values. See documentation of this package in the link provided above.

**PLEASE NOTE: Due to resource constraints, I will only work with a small subset of the whole data set. Specifically, I will randomly sample 4400 rows (10% of the original data set) for this project. Using the entire data set would result in extremely long run times for some of the more complex ML algorithms such as 'AdaBoost' and 'Ensemble'**.

Given the large amount of specific cleaning tasks, I manually went through the documentation, retrieved all the information needed for the cleaning and removed all irrelevant variables and saved the cleaned data set as 'ess8_subsample.csv'. 

```{r data-peak-2, warning=FALSE, message=FALSE, echo=FALSE}
## From here on after the cleaned and smaller subset of the data will be used
## Load cleaned data set locally
# df <- read.csv("ess8_subsample.csv") # If you download the data from edX

## Download data file from my github
df <- read.csv(curl('https://raw.githubusercontent.com/FUenal/Harvard_Capstone_Project/main/ess8_subsample.csv'))

## Random sample for testing purposes: You can choose to work with an even smaller subsample to have shorter run times
# df <- sample_n(df, 500)

## First peak into cleaned data
df_str(df)

# df_str(df, return = "skimr")
```

As visible below, the cleaned data set now contain fewer missing values but still a high number of columns (120). There are still are large number of missing values present: `r sum(is.na(df))`. 

Before proceeding to the next section, I will thus further process the data by identifying features with missing values above 50% and so-called zero-variance features and remove them. As shown below, several features contain more than 50% missing values. I am going to drop these from the data set following general recommendation on dealing with missing values. Note that an alternative way to deal with missing values would be to impute them using mean/median/mode imputation techniques. However, for the purpose of this project it is sufficient to simply remove them. 

```{r data-peak-3, warning=FALSE, message=FALSE, echo=FALSE}
## Check missing values
highmiss <- missingness(df)
head(highmiss)

# Store columns with NaNs > 50%
highmiss50 <- highmiss %>%
        filter(missingness >= 50)

dropmiss50 <- highmiss50[["variable"]]

# Removing variables with NaNs >50% 
df <- df %>%
        select(-one_of(dropmiss50))
```

Checking again for missing values, we can now see that we have less missing values overall (`r sum(is.na(df))`) and that the remaining features all have missing values below 50%

```{r data-peak-4, warning=FALSE, message=FALSE, echo=FALSE}
# Recheck missing values
df_str(df)
head(missingness(df))
```

Checking zero-variance features indicates that non of the remaining features show zero variance and thus I don't have remove any feature based on this criterion.

```{r data-peak-5, warning=FALSE, message=FALSE, echo=FALSE}
# Check zero variance columns (same value in each column)
zerovar(df)
```

Moreover, I will check for missing values in the main variable of interest for this project, the outcome variable 'wrclmch' (Worry Climate Change). As shown below, the response variable contains` missing values. 

```{r data-peak-6, warning=FALSE, message=FALSE, echo=FALSE}
# Check Missing Values in Response Variable
sum(is.na(df$wrclmch))

# Drop NAs in response variable
df <- df %>%
        drop_na(wrclmch)
```

Dropping these will not result in a significantly smaller data set so I will drop all rows containing missing values for the response variable as well.

Next, I will transform the response variable from an ordinary scale type (1: Not Worried to 5: Very Worried) to a binary scale type, to allow for a classification approach. The 'wrclmch' response variable will thus be categorized in 'Not Worried' vs. 'Worried' from here on after. Alternatively, I could have continued to use the outcome as ordinal variable and use regression ML techniques. However, for the sake of simplicity in interpreting the results, it seems more pertinent, to approach the response variable as a binary entity.


```{r transform-response-binarize, warning=FALSE, message=FALSE, echo=FALSE}
# Binarize response variable
df <- df %>%
        mutate(wrclmch = case_when(wrclmch <= 2 ~ "Not_worried",
                                   wrclmch >= 3 ~ "Worried"),
               wrclmch = factor(wrclmch, levels = c("Not_worried", "Worried")))
```

As shown below, the response variable is now coded as a binary variable.

```{r eda-2, warning=FALSE, message=FALSE, echo=FALSE}
# Table response variable counts 
table(df$wrclmch)
```

Similarly, I will also factorize and re-label the gender feature in order to improve the visualization in the coming sections.

```{r transform-gender-binary, warning=FALSE, message=FALSE, echo=FALSE}
## Factor Gender
df <- df %>% 
  mutate(gndr = case_when(gndr <= 1 ~ "Male",
                          gndr >= 2 ~ "Female"),
         gndr = factor(gndr, levels = c("Male", "Female")))

table(df$gndr)
```


As final steps, I will remove the 'idno' column because it is not necessary for my analytic purpose. 

```{r drop-idno, warning=FALSE, message=FALSE, echo=FALSE}
# drop idno 
df <- df[,-1]
```


This data cleaning process is a good example in that it shows that real world data can be quite messy and require some effort to bring into shape such that it is ready for Machine Learning approaches. Also, the example here shows that it is important to use any given information on the data. In this case, the study documentation was very helpful in determining how to best clean the data. Many other real world data sets will not have the luxury of meticulous data documentation and I might count myself lucky in this regard.

After having cleaned the raw data set as described above, I proceed to the next stage of this project: **Exploratory Data Analysis**. 


---


# 4. Exploratory Data Analysis (EDA)

## 4.1. Train - Test Data Split

Before I start with the EDA, I am going to first split the data into **trainData**- and **testData** sets. 

 * **trainData** - a subset (60%) to work with and to develop the models, and 
 
 * **testData** - a subset (40%) to test the final models
 
This is an important step to accomplish prior to EDA since using the entire data set for EDA purposes would obfuscate our train-test set-up by basing our initial understanding of the data on both the **trainData**- and **testData** sets. However, following good Data Science practice requires us to avoid making any decisions based on the test set to avoid so-called data leakage and increase the robustness and generalizability of the results.


```{r train-test-split, warning=FALSE, message=FALSE, echo=TRUE}
# Split data
# Validation set will be 40% of data
set.seed(123, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = df$wrclmch, times = 1, p = 0.4, list = FALSE)
trainData <- df[-test_index,]
testData <- df[test_index,]
```

## 4.2. Frequencies & Distributions

After cleaning and partitioning the data, I proceed to taking a quick glance at the distribution and frequencies of the data. As shown below, some variables show skewed distributions. 

```{r eda-1, warning=FALSE, message=FALSE, echo=FALSE}
## First peak into partitioned trainData
head(df_str(trainData, return = "skimr"))
```

First, let's find out how our response variable is distributed across both the **trainData**- and the **testData**-sets. In other words, let's see how many respondents indicated that they were worried about climate change vs. not worried. The distribution does not indicate a severely high class imbalance and we see that our sample contains more worried than not-worried responses.


**trainData**

```{r eda-2a, warning=FALSE, message=FALSE, echo=FALSE}
# Table response variable counts (1: Not worried to 5: Very worried)
# Train Data
table(trainData$wrclmch) 
```

**testData**

```{r eda-2aa, warning=FALSE, message=FALSE, echo=FALSE}
# Table response variable counts (1: Not worried to 5: Very worried)
# Test Data
table(testData$wrclmch) 
```


Next, we'll continue to analyze the distribution of our outcome variable in the **trainData** set. Let's visualize the distribution of climate change worry first. 

```{r eda-3, warning=FALSE, message=FALSE, echo=FALSE}
# How many worried about climate change?
trainData %>% freqs(wrclmch, plot = T, results = F, abc = TRUE)
```


Next, let's have a look at how our response variable varies across countries. As we can see, there is some  variation in 'being worried or not by climate change' depending on which country respondents are from with most countries having higher scores of 'worried' responses.

```{r eda-4, warning=FALSE, message=FALSE, echo=FALSE}
## climate worry across countries
trainData %>% distr(wrclmch, cntry, clean = TRUE, abc = FALSE)
```

It is reasonable to assume that respondents might also differ in their degree of worry depending on their political leaning. Previous studies have shown that individual more on the right side of the political spectrum tend to see climate change as less of a threat with some even denying that climate change actually happens or is man-made. Let's visualize our response variable across the political spectrum next and find out. The political orientation was measured from 1 (Left) to 10 (Right).

```{r eda-5, warning=FALSE, message=FALSE, echo=FALSE}
## Climate worry across political spectrum
trainData %>% distr(wrclmch, lrscale)
# In politics people sometimes talk of 'left' and 'right'. Using this card, where would you place yourself on this scale, where 0 means the left and 10 means the right
```

Similarly to political orientation, previous studies haven shown that women tend to perceive climate change and environmental problems more worrying on average compared to men. Let's see how climate change worry is distributed across genders next. It appears that climate change worry varies somewhat across genders.

```{r eda-6, warning=FALSE, message=FALSE, echo=FALSE}
## Climate worry across genders (1: Male // 2: Female)
trainData %>% distr(wrclmch, gndr)
```

As seen above, 'climate change worry' varies across different features. We only looked at three features, however, there are more than 120 features in total which we want to explore in more detail. Thus, I will choose another way to gaining insights into the importance of specific features in classifying whether respondents indicated that they were worried vs. not worried by climate change. In the next section, I will first apply some pre-processing steps after which I will consider Feature Selection based on strength of association with the response variable.


---


# 5. Methods & Analysis


## 5.1 Data Pre-processing

Before starting with the pre-processing of the data, I will also remove the 'country' feature, as I am mostly interested in classifying responses across countries.

```{r warning=FALSE, message=FALSE, echo=FALSE}
# head(str(trainData)) # For control purposes only
```

```{r drop-cntry, warning=FALSE, message=FALSE, echo=FALSE}
# drop country
trainData <- trainData[,-1]
```


I will also relocate the position of the response variable 'wrclmch' to the very beginning of the data frame for convenience sake. 

```{r relocate-response-beginning, warning=FALSE, message=FALSE, echo=FALSE, include = FALSE}
# Relocate response variable as first column
fmatch("wrclmch", names(trainData))
trainData <- trainData %>% relocate(wrclmch, .before = gndr)
```

I will also store my response variable and my feature separately for later usage.

```{r sotr-x-y, warning=FALSE, message=FALSE, echo=FALSE}
# Store X and Y for later use.
x = trainData[, 2:123]
y = trainData$wrclmch
```


### 5.1.1 Missing Value Imputation

In the previous sections, we have seen that the data set contains missing values, as I have only removed feature that had more than 50% missing values. For the modeling section, the first thing to implement is to deal with the remaining missing values. There are several approaches to [dealing with missing values](https://scientistcafe.com/ids/missing-values.html). There are many important considerations to make, before starting to handle missing values, such as understanding whether the missing data is missing randomly or whether there is some pattern to the missingness and so on. A sophisticated approach to dealing with missing data is to predict the missing values based on values of the rest of the available variables as predictors. One popular approach that achieves this type of imputation is the k-Nearest Neighbors algorithm. However, the k-Nearest Neighbors algorithm comes at high computational cost and I will not implement it here due to resource constrains. Another common practice is to simply replace missing values with the the mean/median/mode of the column. For continuous features, it is a common practice to replace the missing values with the mean of the column. For categorical features, replace the missing values with the  the mode, the most frequently occurring value is commonly practised. Therefore, for the continuous variables, I will use the preProcess function 'medianImpute' to impute missing values. I am choosing 'median impute' over 'mean impute', because the EDA also indicated that the data is not normally distributed in many cases and show high skewness. In case of skewed data, median imputation is generally recommended.

The caret package offers a convenient preProcess function that handles missing values. Importantly, the same preprocessing we apply to the trainData also needs to be applied to the testData later on. 

```{r preprocessing-1, warning=FALSE, message=FALSE, echo=TRUE}
## Impute Missing Values with medianImpute Method
# Create the median imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method = "medianImpute")
preProcess_missingdata_model
```

As shown below, after imputation of missing values, the data contains no more missing values.

```{r preprocessing-A, warning=FALSE, message=FALSE, echo=TRUE}
# Use the imputation model to predict the values of missing data points
trainData <- predict(preProcess_missingdata_model, newdata = trainData)

# Check for NaNs
anyNA(trainData)
```


### 5.1.2 Normalization

As was shown in the previous sections, the data set contains features with differing scale units and ranges. A common practice in such cases is to normalize data before modeling. The caret package offers an easy way within the preProcess function to apply normalization of data sets, which I am applying next. Below we can see the first 10 columns of the data set indicating that the data has been successfully normalized.

```{r preprocessing-2, warning=FALSE, message=FALSE, echo=TRUE}
## Normalize data
preProcess_range_model <- preProcess(trainData, method='range')
trainData <- predict(preProcess_range_model, newdata = trainData)

# Append the Y variable
trainData$wrclmch <- y

# Show first 10 columns of normalized data
apply(trainData[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
```



### 5.1.3 Feature Selection {.tabset}

After having split the data in **trainData**- and **testData** sets, imputing missing values, and normalization of data, a sensible next step is to reduce the dimensionality of the data. With over 120 features, the data space is still very large even after removing many columns at the very beginning. However, many of these remaining features are probably not at all or not strongly associated with our response variable (i.e., an individuals' perception of climate change as either worrying or not worrying). Moreover, too many features can cause problems such as requiring increased computational power leading to extremely long run-times on standard PCs (such as mine) as well as decreasing efficiency by including variables that have zero or very little predictive power. 

To mitigate these issues, different approaches can be taken which allow to reduce the number of overall features. This process is also referred to as 'Feature Selection'. Common ways of feature selection include Principal Components Analysis (PCA), using built-in methods of feature selection such as 'recursive feature elimination (rfe)', determining feature importance from decision trees or coefficients from Lasso-regularized linear regression to name a few. These approaches require running additional models prior to the main analysis. 

A more convenient and less computationally expensive approach would be to only select features which are highly correlated with the response variable and exclude the remaining features from further analysis. I will take the latter approach due to time and resource constrains and select the 20 highest correlated features. The limitation of this approach however will be to sacrifice complexity in terms of not accounting for interaction terms between features. In other words, I might remove feature which individually are not highly correlated with the response variable but might indicate higher correlation values in interaction with other features. 

Below are the 20 feature which are most highly correlated with the response variable.


```{r preprocessing-3, warning=FALSE, message=FALSE, echo=FALSE}
## Correlation Analysis
# Highest correlation with dependent variable for processing
top20corr <- corr_var(trainData, # name of data set
                      wrclmch, # name of variable to focus on
                      top = 20, # display top 20 correlations
                      ranks = FALSE, # Boolean. Add ranking numbers // MAKE THIS LINE FALSE FOR THE NEXT STEP TO RUN!
                      plot = FALSE # Don't return plot
) 

print(top20corr)

# SELECT TOP 20 CORRELLATIONS
dftopcorr <- top20corr$variables
# str(dftopcorr)

trainData <- trainData %>% 
        select(all_of(dftopcorr))

wrclmch <- y
trainData <- cbind(trainData, wrclmch)
# str(trainData)
```



```{r preprocessing-3a, warning=FALSE, message=FALSE, echo=FALSE, include = FALSE}
# Relocate response variable as first column
fmatch("wrclmch", names(trainData))
trainData <- trainData %>% relocate(wrclmch, .before = wrdpimp)
```

Let's now visualize correlations between our the top 10 selected features and our response variable. The plots below are in line with the correlation analysis we computed further above.

#### Correlation Viz 1

```{r correlation-1, warning=FALSE, message=FALSE, echo=FALSE}
library(ggplot2)
library(GGally)

ggpairs(trainData[,c(2:11,1)], aes(color=wrclmch, alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
        labs(title="Climate Worry Mean")+
        theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))
```

#### Correlation Viz 2

```{r correlation-2, warning=FALSE, message=FALSE, echo=FALSE}
ggcorr(trainData[,c(2:11)], name = "corr", label = TRUE)+
        theme(legend.position="none")+
        labs(title="Climate Worry Mean")+
        theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))
```

#### Correlation Viz 3

```{r correlation-3, warning=FALSE, message=FALSE, echo=FALSE}
library(PerformanceAnalytics)
chart.Correlation(trainData[,c(2:11)],histogram=TRUE, col="grey10", pch=1, main="Climate Worry Mean")
```



---


### 5.1.4 Pre-processing testData

As I have applied the above pre-processing steps on the **trainData**-set alone, I will need to apply the same steps  on the **testData**-set as well. Following the same steps undertaken with the **trainData**, I will 

 * remove the 'country' feature, 
 * relocate the response variable to the very beginning, 
 * store my response variable and my feature separately for later usage, 
 * impute the missing values, 
 * normalize the data, and
 * finally, select the same 20 variables as in the **trainData** set. 

```{r  warning=FALSE, message=FALSE, echo=FALSE}
# head(str(testData)) # For control purposes only
```

```{r drop-cntry2, warning=FALSE, message=FALSE, echo=FALSE}
# drop country
testData <- testData[,-1]
```


```{r relocate-response-beginning2, warning=FALSE, message=FALSE, echo=FALSE, include = FALSE}
# Relocate response variable as first column
fmatch("wrclmch", names(testData))
testData <- testData %>% relocate(wrclmch, .before = gndr)
```

```{r sotr-x-y2, warning=FALSE, message=FALSE, echo=FALSE}
# Store X and Y for later use.
x1 = testData[, 2:123]
y1 = testData$wrclmch
```

```{r preprocessing-1a, warning=FALSE, message=FALSE, echo=FALSE, include = FALSE}
## Prepare the test data set and predict
# Step 1: Impute missing values 
testData2 <- predict(preProcess_missingdata_model, testData)  

# View
# head(testData2[, 1:10])
anyNA(testData2)
```

```{r preprocessing-2a, warning=FALSE, message=FALSE, echo=FALSE}
## Normalize data
# Step 1: Transform the features to range between 0 and 1
testData3 <- predict(preProcess_range_model, testData2)
```

```{r preprocessing-3b, warning=FALSE, message=FALSE, echo=FALSE}
## Highest correlation with dependent variable
## SELECT TOP 20 CORRELLATIONS FROM TRAINDATA
testData3 <- testData3 %>% 
        select(all_of(dftopcorr))

wrclmch <- y1
testData3 <- cbind(testData3, wrclmch)

str(testData3)
```

```{r preprocessing-3ab, warning=FALSE, message=FALSE, echo=FALSE, include = FALSE}
# Relocate response variable as first column
fmatch("wrclmch", names(testData3))
testData3 <- testData3 %>% relocate(wrclmch, .before = wrdpimp)
```



---



## 5.2 Modeling

As I mentioned in the beginning, I will employ a number of Machine Learning Algorithms and compare the results of them to determine the best algorithm. I am taking this broader approach since the underlying motivation of this project is more training and showcasing of skills, rather than employing a model which will inform real-world decision making. I will employ 10 algorithms in total. These algorithms are quite commonly used and represent some of the most up to date and powerful algorithms available on the market. Below is a list of all ML algorithms tested in this project:

### 5.2.1 Model Overview  {.tabset}

 * C5.0 
 
 * KNN 
 
 * SVM 
 
 * naiveBayes 
 
 * rpart 
 
 * ctree 
 
 * Random Forest 
 
 * adaBOOST 
 
 * GBM 
 
 * Ensemble 


#### C5.0 

As a first ML algorithm, I will compute a C5.0 Decision Tree Algorithm. More information on this model can be found under the link below:

 * [C5.0](https://rpubs.com/cyobero/C50)
 
In the first run, I will run a model without parameter tuning. 
 
```{r c50, warning=FALSE, message=FALSE, echo=FALSE}
## C5.0 
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

library(C50)
learn_c50 <- C5.0(trainData[,-1],trainData$wrclmch)
pre_c50 <- predict(learn_c50, testData3[,-1])
cm_c50 <- confusionMatrix(pre_c50, testData3$wrclmch)
cm_c50
```

As shown above, the C5.0 model yielded an accuracy score of `r cm_c50$overall[1]`.




---




#### C5.0 Tuned

Let's now see if we can improve the accuracy by testing different hypertuning parameter values. The plot below shows the optimal number of trials for the best accuracy score and we will subsequently run a model with this value. 

```{r c50-tune, warning=FALSE, message=FALSE, echo=FALSE}
## C5.0 Tune 
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

acc_test <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL

for(i in 1:50){
        learn_imp_c50 <- C5.0(trainData[,-1],trainData$wrclmch,trials = i)      
        p_c50 <- predict(learn_imp_c50, testData3[,-1]) 
        accuracy1 <- confusionMatrix(p_c50, testData3$wrclmch)
        accuracy2[i] <- accuracy1$overall[1]
}

acc <- data.frame(t= seq(1,50), cnt = accuracy2)

opt_t <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of trials is", opt_t$t, "(accuracy :", opt_t$cnt,") in C5.0")

# Plot
library(highcharter)
hchart(acc, 'line', hcaes(t, cnt)) %>%
        hc_title(text = "Accuracy With Varying Trials (C5.0)") %>%
        hc_subtitle(text = sub) %>%
        hc_add_theme(hc_theme_google()) %>%
        hc_xAxis(title = list(text = "Number of Trials")) %>%
        hc_yAxis(title = list(text = "Accuracy"))
```


```{r c50-tuned, warning=FALSE, message=FALSE, echo=FALSE}
### Apply optimal trials to show best predict performance in C5.0
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

learn_imp_c50 <- C5.0(trainData[,-1],trainData$wrclmch,trials=opt_t$t)    
pre_imp_c50 <- predict(learn_imp_c50, testData3[,-1])
cm_imp_c50 <- confusionMatrix(pre_imp_c50, testData3$wrclmch)
cm_imp_c50
```

As shown above, a model with optimized parameters (`r opt_t$t` Trials) yielded a slightly better accuracy score of `r cm_imp_c50$overall[1]` compared to the previous model in which I did not apply hyperparameter tuning.



---




#### knn

Next, I will compute a K-Nearest Neighbors (KNN) model. More information on this model can be found under the link below:

 * [KNN](https://rafalab.github.io/dsbook/examples-of-algorithms.html#k-nearest-neighbors)
 
As in the model before, I will evaluate the accuracy by testing different hypertuning parameter values. The plot below shows the optimal number of `k` yielding the best accuracy score. Thus, I will subsequently run a model with this value specification.
 
```{r knn, warning=FALSE, message=FALSE, echo=FALSE}
## KNN - Tune 
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

library(class)

acc_test <- numeric() 

for(i in 1:30){
        predict <- knn(train=trainData[,-1], test=testData3[,-1], cl=trainData[,1], k=i, prob=T)
        acc_test <- c(acc_test,mean(predict==testData3[,1]))
}

acc <- data.frame(k= seq(1,30), cnt = acc_test)

opt_k <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of k is", opt_k$k, "(accuracy :", opt_k$cnt,") in KNN")

# Plot
hchart(acc, 'line', hcaes(k, cnt)) %>%
        hc_title(text = "Accuracy With Varying K (KNN)") %>%
        hc_subtitle(text = sub) %>%
        hc_add_theme(hc_theme_google()) %>%
        hc_xAxis(title = list(text = "Number of Neighbors(k)")) %>%
        hc_yAxis(title = list(text = "Accuracy"))
```


```{r knn1, warning=FALSE, message=FALSE, echo=FALSE}
## Apply optimal K to show best predict performance in KNN
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

pre_knn <- knn(train = trainData[,-1], test = testData3[,-1], cl = trainData[,1], k=opt_k$k, prob=T)
cm_knn  <- confusionMatrix(pre_knn, testData3$wrclmch)
cm_knn
```

As shown above, a model with optimized number of (`r opt_k$k` `k`) yielded an accuracy score of `r opt_k$cnt` which we determined above as best score between 1 to 30 `k` values.




---


#### naiveBayes 

The next ML algorithm I will compute is a Naive Bayes model. More information on this model can be found under the link below:

 * [Naive Bayes](https://rafalab.github.io/dsbook/examples-of-algorithms.html#naive-bayes)
 
I will run a model re-iterating over 1 to 30 in ‘laplace’ values. 

```{r naivebayes, warning=FALSE, message=FALSE, echo=FALSE}
library(e1071)

# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

acc_test <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL

for(i in 1:30){
        learn_imp_nb <- naiveBayes(trainData[,-1], trainData$wrclmch, laplace=i)    
        p_nb <- predict(learn_imp_nb, testData3[,-1]) 
        accuracy1 <- confusionMatrix(p_nb, testData3$wrclmch)
        accuracy2[i] <- accuracy1$overall[1]
}

acc <- data.frame(l= seq(1,30), cnt = accuracy2)

opt_l <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of laplace is", opt_l$l, "(accuracy :", opt_l$cnt,") in naiveBayes")

# Plot
hchart(acc, 'line', hcaes(l, cnt)) %>%
        hc_title(text = "Accuracy With Varying Laplace (naiveBayes)") %>%
        hc_subtitle(text = sub) %>%
        hc_add_theme(hc_theme_google()) %>%
        hc_xAxis(title = list(text = "Number of Laplace")) %>%
        hc_yAxis(title = list(text = "Accuracy"))
```

As visible in the plot above, varying the  ‘laplace’ values does not improve (nor diminish) the prediction of the model. Thus, I will run a naiveBayes model without specific ‘laplace’ specification. 

```{r naivebayes1, warning=FALSE, message=FALSE, echo=FALSE}
# naiveBayes without laplace
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

learn_nb <- naiveBayes(trainData[,-1], trainData$wrclmch)
pre_nb <- predict(learn_nb, testData3[,-1])
cm_nb <- confusionMatrix(pre_nb, testData3$wrclmch)        
cm_nb
```

The results of this model are shown in the confusion matrix above. The model yielded an accuracy score of `r opt_l$cnt`



 

---



#### SVM

Next, I will compute a Support Vector Machine (SVM) model. More information on this model can be found under the link below:

 * [SVM](https://en.wikipedia.org/wiki/Support-vector_machine)
 
Let's first try a model without hyperparameter tuning.

```{r svm, warning=FALSE, message=FALSE, echo=FALSE}
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

## SVM 
learn_svm <- svm(wrclmch~., data=trainData)
pre_svm <- predict(learn_svm, testData3[,-1])
cm_svm <- confusionMatrix(pre_svm, testData3$wrclmch)
cm_svm
```

The confusion matrix visible above indicates an accuracy score of `r cm_svm$overall[1]`. Let's see if we can improve upon this score by fine tuning some hyperparameters. Specifically, we are going to grid search over a range of different `gamma` and `cost` values.



---



#### SVM Tune 
```{r svm-tune, warning=FALSE, message=FALSE, echo=FALSE}
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

## SVM -Tune 
### Choose ‘gamma, cost’ which shows best predict performance in SVM

gamma <- seq(0,0.1,0.005)
cost <- 2^(0:5)
parms <- expand.grid(cost=cost, gamma=gamma)    ## 231

acc_test <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL

for(i in 1:NROW(parms)){        
        learn_svm <- svm(wrclmch~., data=trainData, gamma=parms$gamma[i], cost=parms$cost[i])
        pre_svm <- predict(learn_svm, testData3[,-1])
        accuracy1 <- confusionMatrix(pre_svm, testData3$wrclmch)
        accuracy2[i] <- accuracy1$overall[1]
}

acc <- data.frame(p= seq(1,NROW(parms)), cnt = accuracy2)

opt_p <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of parameter is", opt_p$p, "(accuracy :", opt_p$cnt,") in SVM")

# Plot
hchart(acc, 'line', hcaes(p, cnt)) %>%
        hc_title(text = "Accuracy With Varying Parameters (SVM)") %>%
        hc_subtitle(text = sub) %>%
        hc_add_theme(hc_theme_google()) %>%
        hc_xAxis(title = list(text = "Number of Parameters")) %>%
        hc_yAxis(title = list(text = "Accuracy"))
```

As shown in the plot above, the optimal number of parameters seems to be `r opt_p$p` yielding an accuracy score of `r opt_p$cnt`. We will use this value for our optimized model. 

```{r svm-tune1, warning=FALSE, message=FALSE, echo=FALSE}
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

### Apply optimal parameters(gamma, cost) to show best predict performance in SVM

learn_imp_svm <- svm(wrclmch~., data=trainData, cost=parms$cost[opt_p$p], gamma=parms$gamma[opt_p$p])
pre_imp_svm <- predict(learn_imp_svm, testData3[,-1])
cm_imp_svm <- confusionMatrix(pre_imp_svm, testData3$wrclmch)
cm_imp_svm
```

The confusion matrix above summarizes the results for the optimized SVM model. The accuracy score of the tuned model (`r cm_imp_svm$overall[1]`) is slightly better than the non-tuned model (`r cm_svm$overall[1]`).





---



#### rpart

Next, I will compute a Recursive Partitioning (rpart) model. More information on this model can be found under the link below:

 * [rpart](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)

For the rpart model, I will use 5-fold cross validation with the caret packages' in-built `trainControl` function. As hyperparameter tuning specifications, I will use the built-in function of the `caret` package by specifying the `tuneLength` argument to 2. The 'tuneLength' parameter tells the algorithm to try different default values for the main parameters, in our case 2. I will use both the `tuneLength` and the `trainControl` functions for the following models as well.

```{r hyperparameter-tuning2, warning=FALSE, message=FALSE, echo=FALSE}
## Hyperparameter tuning to optimize the model for better performance
# 1. trainControl()

# Define the training control
fitControl <- trainControl(
        method = 'cv',                   # k-fold cross validation
        number = 5,                      # number of folds
        savePredictions = 'final',       # saves predictions for optimal tuning parameter
        classProbs = T,                  # should class probabilities be returned
        summaryFunction=twoClassSummary  # results summary function
) 
```


```{r rpart-caret, warning=FALSE, message=FALSE, echo=FALSE}
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

# Train the model using rpart
model_rpart = train(wrclmch ~ ., data=trainData, method='rpart', tuneLength=2, trControl = fitControl)
# model_rpart

# Predict on testData and Compute the confusion matrix
predicted5 <- predict(model_rpart, testData3)
confmat_rpart <- confusionMatrix(reference = testData3$wrclmch, data = predicted5)
confmat_rpart
```


The `rpart` model confusion matrix is shown above.


---



#### ctree

Next, I will compute a conditional inference trees (CTree) model. More information on this model can be found under the link below:

 * [ctree](https://cran.r-project.org/web/packages/partykit/vignettes/ctree.pdf)
 
 
```{r ctree-caret, warning=FALSE, message=FALSE, echo=FALSE}
## Training ctree
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

# Train the model using ctree
model_ctree = train(wrclmch ~ ., data=trainData, method='ctree', tuneLength=2, trControl = fitControl)
# model_ctree

# Predict on testData and Compute the confusion matrix
predicted6 <- predict(model_ctree, testData3)
confmat_ctree <- confusionMatrix(reference = testData3$wrclmch, data = predicted6)
confmat_ctree
```

Using again 5-fold cross-validation and setting `tuneLength` argument to 2, the `ctree` model yielded the above results.


---



#### Random Forest 

Next, I will compute a popular and widely used Random Forest model. More information on this model can be found under the link below:

 * [Random Forest](https://rafalab.github.io/dsbook/examples-of-algorithms.html#random-forests)
 
```{r rf-caret, warning=FALSE, message=FALSE, echo=FALSE}
## Training Random Forest
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

# Train the model using rf
model_rf = train(wrclmch ~ ., data=trainData, method='rf', tuneLength=2, trControl = fitControl)
# model_rf

# Predict on testData and Compute the confusion matrix
predicted7 <- predict(model_rf, testData3)
confmat_rf <- confusionMatrix(reference = testData3$wrclmch, data = predicted7)
confmat_rf
```

The results for the `Random Forest Model` are shown above.


---


#### GBM

Next, I will compute a Gradient boosting (GBM) model. More information on this model can be found under the link below:

 * [GBM](https://en.wikipedia.org/wiki/Gradient_boosting)
 

```{r gbm-caret, warning=FALSE, message=FALSE, echo=FALSE, include=F}
## Training GBM
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

# Train the model using GBM
model_gbm = train(wrclmch ~ ., data=trainData, method='gbm', tuneLength=2, trControl = fitControl)
# model_gbm

# Predict on testData and Compute the confusion matrix
predicted8 <- predict(model_gbm, testData3)
confmat_gbm <- confusionMatrix(reference = testData3$wrclmch, data = predicted8)

```

```{r gbm-caret-plot, warning=F, message=F, echo=F}
confmat_gbm
```


The results for the `GBM` model are shown above.

 
---


#### adaBOOST

Next, I will compute a Adaptive Boosting (adaBOOST)  which is a statistical classification meta-algorithm model. More information on this model can be found under the link below:

 * [adaBoost](https://en.wikipedia.org/wiki/AdaBoost)
 
  
```{r adaboost-caret, warning=FALSE, message=FALSE, echo=FALSE}
##Training Adaboost
# Set the seed for reproducibility
set.seed(123, sample.kind="Rounding")

# Train the model using Adaboost
model_adaboost = train(wrclmch ~ ., data=trainData, method='adaboost', tuneLength=2, trControl = fitControl)
# model_adaboost

# Predict on testData and Compute the confusion matrix
predicted9 <- predict(model_adaboost, testData3)
confmat_ada <- confusionMatrix(reference = testData3$wrclmch, data = predicted9)
confmat_ada
```

The results for the `AdaBoost` model are shown above.



---



#### Ensemble

In the final step, I will Ensemble the predictions of all `models` that I have run so far to form a new combined prediction based on `glm`. More information on this model can be found under the links below:

 * [Ensemble](https://en.wikipedia.org/wiki/Ensemble_learning)
 
 * [GLM for Ensemble](https://machinelearningmastery.com/machine-learning-ensembles-with-r/)
 


Up until here, I have run several ML algorithms individually. However, the `caretEnsemble` package also allows us to run all the different ML models in one call using the `caretEnsemble::caretList()` function instead of the `caret::train()` function. This is what I am going to employ next. As a first step for the Ensemble model, I will start defining the cross-validation method and the desired summary statistics using the `trainControl` function.  Then I am going to create a vector which contains the names of the all the algorithms I want to run. These are going to be the same ones as I have employed before and therefore, when I run the Ensemble model in the next step, I will be able ascertain whether the Ensemble model yields a better accuracy score compared to the individual models. 


```{r caret-ensemble1, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
### Ensembling the predictions
library(caretEnsemble)
# Stacking Algorithms - Run multiple algorithms in one call.
trainControl <- trainControl(method="repeatedcv", 
                             number=5, 
                             repeats=2,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

algorithmList <- c('knn', 'naive_bayes', 'C5.0', 'rpart', 'ctree',  'rf', 'gbm', 'adaboost', 'svmRadial')

set.seed(123, sample.kind="Rounding")
models <- caretList(wrclmch ~ ., data=trainData, trControl=trainControl, methodList=algorithmList) 
results <- resamples(models)
summary(results)
```

The plots above show the accuracy and Kappa results for all models in the Ensemble individually.

```{r caret-ensemble-plot, warning=FALSE, message=FALSE, echo=FALSE}
# Box plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)
```


The next step is then to combine the predictions of all our models to form a final prediction using a Generalized Linear Model algorithm (GLM). Below, the results for the Ensemble GLM model are shown.


```{r caret-ensemble2, warning=FALSE, message=FALSE, echo=FALSE}
### Combine the predictions of multiple models to form a final prediction
# Create the trainControl
set.seed(124, sample.kind="Rounding")
stackControl <- trainControl(method="repeatedcv", 
                             number=5, 
                             repeats=2,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

# Ensemble the predictions of `models` to form a new combined prediction based on glm
stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)
stack.glm

# Predict on testData and Compute the confusion matrix
predicted12 <- predict(stack.glm, newdata=testData3)
confmat_glm <- confusionMatrix(reference = testData3$wrclmch, data = predicted12, positive = 'Worried')
```



---



### 5.2.2 Model Accuracy Comparison

Let's now take a closer look at the accuracy scores of all the models that I have run by visualizing the accuracy scores side-by-side. 

```{r model-accuracies, warning=FALSE, message=FALSE, echo=FALSE}
## Visualize to compare the accuracy of all methods
col <- c("#ed3b3b", "#0099ff")
par(mfrow=c(4,4))
fourfoldplot(cm_c50$table, color = col, conf.level = 0, margin = 1, main=paste("C5.0 (",round(cm_c50$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_imp_c50$table, color = col, conf.level = 0, margin = 1, main=paste("Tune C5.0 (",round(cm_imp_c50$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_knn$table, color = col, conf.level = 0, margin = 1, main=paste("KNN (",round(cm_knn$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_nb$table, color = col, conf.level = 0, margin = 1, main=paste("NaiveBayes (",round(cm_nb$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_svm$table, color = col, conf.level = 0, margin = 1, main=paste("SVM (",round(cm_svm$overall[1]*100),"%)",sep=""))
fourfoldplot(cm_imp_svm$table, color = col, conf.level = 0, margin = 1, main=paste("Tune SVM (",round(cm_imp_svm$overall[1]*100),"%)",sep=""))
fourfoldplot(confmat_rpart$table, color = col, conf.level = 0, margin = 1, main=paste("RPart (",round(confmat_rpart$overall[1]*100),"%)",sep=""))
fourfoldplot(confmat_ctree$table, color = col, conf.level = 0, margin = 1, main=paste("CTree (",round(confmat_ctree$overall[1]*100),"%)",sep=""))
fourfoldplot(confmat_rf$table, color = col, conf.level = 0, margin = 1, main=paste("RandomForest (",round(confmat_rf$overall[1]*100),"%)",sep=""))
fourfoldplot(confmat_gbm$table, color = col, conf.level = 0, margin = 1, main=paste("GBM (",round(confmat_gbm$overall[1]*100),"%)",sep=""))
fourfoldplot(confmat_ada$table, color = col, conf.level = 0, margin = 1, main=paste("AdaBoost (",round(confmat_ada$overall[1]*100),"%)",sep=""))
fourfoldplot(confmat_glm$table, color = col, conf.level = 0, margin = 1, main=paste("Ensemble GLM (",round(confmat_glm$overall[1]*100),"%)",sep=""))
```



```{r best-model-accuracy, message=FALSE, warning=FALSE, echo=FALSE}
opt_predict <- c(cm_c50$overall[1], cm_imp_c50$overall[1], cm_knn$overall[1], cm_nb$overall[1], cm_svm$overall[1], cm_imp_svm$overall[1], 
                 confmat_rpart$overall[1], confmat_ctree$overall[1], confmat_rf$overall[1], confmat_gbm$overall[1], confmat_ada$overall[1], confmat_glm$overall[1])
names(opt_predict) <- c('c50', 'imp_c50', 'knn', 'nb', 'svm' , 'imp_svm', 'rpart', 'ctree', 'rf', 'gbm', 'ada', 'glm')
best_predict_model <- subset(opt_predict, opt_predict==max(opt_predict))
```

As shown in the plots above, the individual model which achieved the best accuracy score used on the test set has a accuracy score of `r best_predict_model`. 



---
 
 

# 6. Results

Looking at the results of the algorithms individually, the plot below depicts the performance of the models in terms of accuracy:

```{r model_comparison, message=FALSE, warning=FALSE, echo=FALSE}
# Box plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)
```

The best accuracy score(-s) used on the test set achieved an accuracy of:

```{r best-model-accuracy1, message=FALSE, warning=FALSE, echo=FALSE}
best_predict_model
```

In contrast, using an Ensemble model yielded an accuracy as shown below.

```{r glm-results, message=FALSE, warning=FALSE, echo=FALSE}
stack.glm
```


As a final step, let`s now take a closer look at the specific variables which were most important in the prediction of on of the models. The plot below shows the most important features in the classification of our response variable.

```{r varImp, message=FALSE, warning=FALSE, echo=FALSE}
varimp_rf <- varImp(model_rf)
plot(varimp_rf, main="Variable Importance with Random Forest")
```





---



# 7. Conclusion

Here, I reported the results of the second part of my **HarvardX PH125.9x Data Science: Capstone Project**. The goal of this full-stack data science project was to download, wrangle, explore, and analyze European Citizens' perceptions of climate change using 10 different Machine Learning (ML) Algorithms (*KNN, naiveBayes, Random Forest, adaBOOST, SVM, Ensemble, ctree, GBM, C5.0, rpart*) while also employing hyperparameter tuning (e.g., *tuneGrid* and *tuneLength*) to some of the models to improve the results. 

The data for this project came from the 8th round [European Social Survey (2016)](https://www.europeansocialsurvey.org/data/download.html?r=8), a publicly available academic data set. The response variable (outcome) in this project was "climate change worry", i.e., whether or not individuals in more than 30 European countries are worried about climate change. The response to this variable was binary ('yes' vs. 'no') and thus, this project dealt with a classification problem

After a contextual introduction into the topic of climate change perceptions, I explained how the data download, selection, and cleaning was done. Then, after splitting the data into *train* and *test* sets, I proceeded with an Exploratory Data Analysis (EDA) in which I explored the train data set and visualized the initial data exploration gaining some insights for modeling. In the Methods & Analysis Section, I employed some feature engineering beyond the steps undertaken in the initial data selection process (i.e., missing value imputation, removing zero variance features, normalization of data). Afterwards, I employed several ML algorithms and checked to what extent each of the algorithms accurately classifies the response variable. 

Out of the 10 individual ML algorithms tested, the best results was (were) achieved by 

```{r best-model-accuracy1a, message=FALSE, warning=FALSE, echo=FALSE}
best_predict_model
```


## Limitations and future directions

While the final accuracy score is quite high and satisfactory by comparison to different academic papers dealing with similar issues, several other methods and techniques could be considered to further improve the results. First, as I mentioned further above, I chose to reduce the complexity of the data set by selecting the 20 most strongly correlated features. The reason for this approach was purely resource centred and alternative approaches which are computationally more expensive such as Principal Components Analysis or coefficients from Lasso-regularized linear regression could be employed to further improve the feature selection approach. Moreover, for most of the models, I chose to employ the built-in hyperparameter tuning functionality of the `caret` package. This is not a wrong approach, however, it is less flexible and a manual tuning of hyperparameters by searching over specific parameters might even further improve the accuracy of the models. Furthermore, I chose accuracy as my main evaluation metric. However, besides accuracy, I also could have focused on Sensitivity,  Specificity, ROC as further evaluation metrics. As was shown above, the models yielded considerably different results for these metrics. Depending on the classification problem, considering Sensitivity and Specificity might be more important than overall Accuracy (e.g., Cancer detection, Plane Malfunction, etc.). Lastly, I have only used a randomly picked subset of the entire data set. The results could be more reliable if the entire data set should be used. However, given the relatively large size of the data set, one should keep in mind though that some of the ML algorithms used here would require high computational processing power and as such, a regular laptop, such as the one I used in my project requires would require several hours to process the models, without sufficient computing power. 

Overall, the project was fun and interesting to accomplish and I hope that my results and approach will be useful for others in their own learning journeys. The topic of climate change is an urgent and important matter. I hope that by using cutting edge and powerful ML techniques, I was able to contribute to research in this area. Please note, that some of the materials presented here are also in preparation for submission to an academic, peer-reviewed journal. 

If you are interested in finding out more about the data used in this project, the data set is publicly available following the below link:

 * [European Social Survey 8 (2016)](https://www.europeansocialsurvey.org/data/download.html?r=8) 




**In closing, I would like to thank Prof Rafael Irizarry and his team at Harvard University, as well as the edX team for this very rewarding and comprehensive learning experience!** 





## Session Info 

```{r message=FALSE, warning=FALSE, echo=FALSE}
sessionInfo(package = NULL)
```



## Benchmark: time to generate Pdf from Rmd 
 
 | Machine                | Time    |
 |------------------------|--------:|
 | MacBook Pro        8GB |  25'41  |



